{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Optimization for Traning Deep Models\n",
    "\n",
    "## 8.1. How Learning Differs from Pure Optimization\n",
    "\n",
    "In most machine learning scenarios, we care about some performance measure P, that is deﬁned with respect to the **test set** and may also be intractable.\n",
    "We therefore **optimize P only indirectly**. We reduce a diﬀerent cost function J(θ) in the hope that doing so will improve P. \n",
    "\n",
    "<center> $J(\\theta) = \\mathbb{E}_{(x,y)\\sim\\hat{p}_{data}}L(f(x;\\theta),y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Empirical Risk Minimization\n",
    "\n",
    "The goal of a machine learning algorithm is to reduce the expected generalization error given by equation. This quantity is known as the **risk**\n",
    "\n",
    "However, when **we do not know $p_{data}(x,y)$ ** but only have a training set of samples, we have a machine learning problem\n",
    "\n",
    "The simplest way to convert a machine learning problem back into an optimization problem is **to minimize the expected loss on the training set.**\n",
    "\n",
    "<center>$ \\cfrac{1}{m}\\sum^m_{i=1}L(f(x^{(i)};\\theta), y^{(i)})$\n",
    "\n",
    "m is the number of training examples\n",
    "\n",
    "- However, **empirical risk minimization is prone to overﬁtting**\n",
    "\n",
    "- The most eﬀective modern optimization algorithms are based on gradient descent, but many useful loss functions, such as 0-1 loss, **have no useful derivatives** (the derivative is either zero or undeﬁned everywhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. Surrogate Loss Functions and Early Stopping\n",
    "\n",
    "a surrogate loss function instead, which **acts as a proxy but has advantages. **\n",
    "\n",
    "For example, **the negative log-likelihood** of the correct class is typically used as a surrogate for the 0-1 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Batch and Minibatch Algorithms\n",
    "\n",
    "In practice, we can compute these expectations by **randomly sampling a small number of examples from the dataset**, then taking the average over only those examples\n",
    "\n",
    "- **converge much faster** (in terms of total computation, not in terms of number of updates) if they are allowed to rapidly **compute approximate estimates of the gradient rather than slowly computing the exact gradient**\n",
    "\n",
    "- Another consideration motivating statistical estimation of the gradient from a small number of samples is **redundancy in the training set**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use the entire training set : **batch or deterministic** gradient methods\n",
    "- use only a single example at a time :  **stochastic** or sometimes **online** methods.\n",
    "- **use more than one but less than all of the training examples : minibatch or minibatch stochastic** methods and it is now common to simply call them stochastic methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch sizes are generally driven by the following factors:\n",
    "- Larger batches provide a more accurate estimate of the gradient, but with less than linear returns. \n",
    "- Multicore architectures\n",
    "- to be processed in parallel, then the amount of memory scales with the batch size\n",
    "- **Small batches can oﬀer a regularizing eﬀect**\n",
    "    - Generalization error is often best for a batch size of 1.\n",
    "    - Training with such **a small batch size **might **require a small learning rate** to maintain stability due to the high variance in the estimate of the gradient.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Methods that compute updates based **only on the gradient g** are usually relatively robust and can handle **smaller batch sizes like 100**\n",
    "- **Second-order methods, which use also the Hessian matrix** H require much larger batch sizes like **10,000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Challenges in Neural Network Optimization\n",
    "When training neural networks, we must confront the general non-convex case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 수치해석 분야에서 함수의 조건수(condition number)는 argument에서 의 작은 변화의 비율에 대해 함수가 얼마나 변화할 수 있는지에 대한 argument measure이다. (https://ko.wikipedia.org/wiki/%EC%A1%B0%EA%B1%B4%EC%88%98)\n",
    "\n",
    "### 8.2.1 Ill-Conditionin\n",
    "- **The ill-conditioning problem is generally believed to be present in neural network training problems.**\n",
    "- Ill-conditioning can manifest by causing SGD to get “stuck” in the sense that even very small steps increase the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a second-order Taylor series expansion \n",
    "<center>$ \\cfrac{1}{2}\\epsilon^2g^THg - \\epsilon g^Tg$\n",
    "\n",
    "Ill-conditioning of the gradient becomes a problem  **when $ \\cfrac{1}{2}\\epsilon^2g^THg$ exceeds $\\epsilon g^Tg$.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases, the gradient norm : $g^Tg$ does not shrink signiﬁcantly throughout learning, \n",
    "\n",
    "- but the $g^THg$ term grows by more than an order of magnitude\n",
    "\n",
    "The result is that **learning becomes very slow despite the presence of a strong gradient because the learning rate must be shrunk to compensate for even stronger curvature **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Local Minima\n",
    "- Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the model **identiﬁability problem**\n",
    "\n",
    "- Models with latent variables are often **not** identiﬁable **because we can obtain equivalent models by exchanging latent variables with each other** (weight space symmetry)\n",
    "\n",
    "- This means that **there can be an extremely large or even uncountably inﬁnite amount of local minima **\n",
    "\n",
    "- Nowdays The problem is ** to ﬁnd a point in parameter space that has low but not minimal cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 Plateaus, Saddle Points and Other Flat Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4 Cliﬀs and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5 Long-Term Dependencies\n",
    "when the computational graph becomes extremely deep\n",
    "\n",
    "suppose that a computational graph contains a path that consists of repeatedly multiplying by a matrix $W$.\n",
    "\n",
    "**After t steps**, this is equivalent to multiplying by $W^t$.\n",
    "\n",
    "Suppose that W has an eigendecomposition $W = V\\text{diag}(λ)V^{−1}$. \n",
    "\n",
    "<center> $W^t = (V\\text{diag}(λ)V^{−1})^t = V\\text{diag}(λ)^tV^{−1}$\n",
    "\n",
    "** vaninshing and exploding gradient problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.6 Inexact Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.7 Poor Correspondence between Local and Global Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.8 Theoretical Limits of Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Basic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Parameter Initialization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Algorithms with Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4 Choosing the Right Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Approximate Second-Order Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1 Newton’s Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.2 Conjugate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.3 BFGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Optimization Strategies and Meta-Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1 Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2 Coordinate Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.3 Polyak Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.4 Supervised Pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.5 Designing Models to Aid Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.6 Continuation Methods and Curriculum Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the weigth decay and solve for the minimum of the regluarized version of $ \\hat{J} $\n",
    "\n",
    "$\\tilde{w}$to represent the location of the minimum\n",
    "\n",
    "<center>$ \\alpha\\tilde{w} + H(w-w^*) = 0 $\n",
    "\n",
    "<center>$ \\tilde{w} = (H+\\alpha I)^{-1}Hw^* $\n",
    "\n",
    "As α approaches 0, the regularized solution $\\tilde{w}$ approaches $w^*$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$ H = Q \\Lambda Q^T $\n",
    "\n",
    "<center>$\\tilde{w} = Q(\\Lambda + \\alpha I)^{-1}\\Lambda Q^Tw^* $\n",
    "\n",
    "Speciﬁcally, the component of w∗ that is aligned with the i-th eigenvector of H is rescaled by a factor of $\\cfrac{\\lambda_i}{\\lambda_i+\\alpha}$\n",
    "\n",
    "**Along the directions where the eigenvalues of H are relatively large, for example, where λi $\\gg$α, the eﬀect of regularization is relatively small. However, components with λi $\\ll$α will be shrunk to have nearly zero magnitude**\n",
    "\n",
    "The corresponding eigenvalue is large, indicating high curvature. As a result, weight decay aﬀects the position of w2 relatively little.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. $L^1$ Regularization\n",
    "\n",
    "<center>$\\Omega(\\theta)=||w||_1=\\sum_i{|w_i|} $\n",
    "\n",
    "<center>$ \\tilde{J}(w; X, y) = \\alpha||w||_1 + J(w; X, y) $\n",
    "\n",
    "<center>$ \\nabla_w \\tilde{J}(w; X, y) = \\alpha $sign$(w) + \\nabla_wJ(X, y;w)  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$ \\nabla_w \\hat{J}(w) = H(w-w^*) $\n",
    "\n",
    "make the further simplifying assumption that the Hessian is diagonal\n",
    "This assumption holds if the data for the linear regression problem has been preprocessed to **remove all correlation between the input features**, which may be accomplished using PCA\n",
    "\n",
    "<center>$ \\tilde J(w;X,y) = J(w^*;X,y) + \\sum_i[\\cfrac{1}{2}H_{i,i}(w_i-w_i^*)^2 + \\alpha|w_i|]$\n",
    "\n",
    "<center>$ w_i = $sign$(w_i^*) \\ $max$ \\left\\{ |w_i^*| - \\cfrac{\\alpha}{H_{i,i}},0\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to L2 regularization, L1 regularization results in a solution that is more **sparse**\n",
    "\n",
    "The sparsity property induced by L1 regularization has been used extensively as a **feature selection** mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Norm Penalties as Constrained Optimization\n",
    "If we wanted to constrain Ω(θ) to be less than some constant , we could construct a ** generalized Lagrange function **\n",
    "\n",
    "<center>$ \\mathcal{L}(\\theta, \\alpha; X, y) = J(\\theta;,X,y) + \\alpha(\\Omega(\\theta)-k) $\n",
    "\n",
    "The Solution\n",
    "\n",
    "<center>$ \\theta^* = \\underset{\\theta}{\\text{arg min}} \\ \\underset{\\alpha, \\alpha\\ge0}{\\text{max}} \\mathcal{L}(\\theta, \\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solving this problem requires modifying both $\\theta$ and $\\alpha$ <br>\n",
    "- $\\alpha$ must increase whenever  $\\Omega(\\theta) > k$ <br>\n",
    "- $\\alpha$ must decrease whenever $\\Omega(\\theta) < k$ <br>\n",
    "- All positive $\\alpha$ encourage $\\Omega(\\theta)$ to shrink\n",
    "- Optimal value $\\alpha^*$ will encourage $\\Omega(\\theta)$ to shrink, but not so strongly to make $\\Omega(\\theta) < k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $k$를 모르므로 $\\alpha$를 사용해서  contrained region 의 크기(size)를 조절한다?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some insight into the eﬀect of the constraint, we can ﬁx α∗ and view the problem as just a function of $\\theta$\n",
    "\n",
    "\n",
    "<center>$ \\theta^* = \\underset{\\theta}{\\text{arg min}} \\ \\mathcal{L}(\\theta, \\alpha^*) \n",
    "= \\underset{\\theta}{\\text{arg min}} \\ J(\\theta; X, y) + \\alpha^*\\Omega(\\theta)$\n",
    "\n",
    "**This is exactly the same as the regularized training problem of minimizing $\\tilde{J}$**\n",
    "\n",
    "We can thus think of a parameter norm penalty as imposing a constraint on the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explicit contraints 라는 것은 k값을 알고 있다는 의미?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also use <strong><em> explicit constraints </em></strong> rather than penalties\n",
    "\n",
    "- can modify SGD to take a step downhill on $J(\\theta)$ and then project $\\theta$ back to the nearest point that satisfies $\\Omega(\\theta) < k $.\n",
    "- useful if we know what value of $k$ is appropriate and don't want to waste time searching for $\\alpha$ that corresponds to this $k$\n",
    "- penalties can cause non-convex optimization procedures to get stuck in local minima corresponding to small $\\theta$\n",
    "- Explicit constraints implemented by re-projection only have an effect when the weights become large and attempt to leave the constraint region\n",
    "- Explicit constraints with reprojection imposes some <strong> stability </strong> on the optimization procedure - prevents positive feedback loop from continuing to increase the magnitude of the weights without bound.\n",
    "- In practice, column norm limitation is always implemented as an explicit constraint with reprojection so as to prevent any one hidden unit from having very large weights. \n",
    "- If we converted this constraint into a penalty in a Lagrange function, it would be similar to $L^2$ weight decay but with a separate KKT multiplier for the weights of each hidden unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Regularization and Under-Constrained Problems\n",
    "In this case, many forms of regularization correspond to inverting $X^TX + \\alpha I$ instead\n",
    "\n",
    "**An iterative optimization procedure like stochastic gradient descent will continually increase the magnitude of w and, in theory, will never halt.**\n",
    "\n",
    "**Most forms of regularization are able to guarantee the convergence of iterative methods applied to underdetermined problems.** For example, weight decay will cause gradient descent to quit increasing the magnitude of the weights when the slope of the likelihood is equal to the weight decay coeﬃcient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Dataset Augmentation\n",
    "## 7.5. Noise Rubustness\n",
    "-  In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when **the noise is added to the hidden units.**\n",
    "- Another way that noise has been used in the service of regularizing models is by **adding it to the weights. **\n",
    "- random perturbation $ \\epsilon W \\sim N(\\epsilon; 0, \\eta I)$. The objective function :\n",
    "<center>$ \\tilde{J}_W = \\mathbb{E}_{p(x,y,\\epsilon W}[(\\hat{y}_{\\epsilon W}(x) - y)^2]$\n",
    "$ = \\mathbb{E}_{p(x,y,\\epsilon W}[\\hat{y}^2_{\\epsilon W}(x) -2y\\hat{y}_{\\epsilon W}(x)+ y^2]$\n",
    "- For small η, the minimization of J with added weight noise (with covariance ηI) is equivalent **to minimization of J with an additional regularization term: **\n",
    "\n",
    "\n",
    "### 7.5.1. Injecting Noise at the Output Targets\n",
    "Most datasets have some amount of mistakes in the y labels. It can be harmful to maximize logp(y | x) when y is a mistake. One way to prevent this is to explicitly **model the noise on the labels. **\n",
    "label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Semi-Supervised Learning\n",
    "\n",
    "In the context of deep learning, semi-supervised learning usually refers to learning a representation h = f(x). \n",
    "\n",
    "The goal is to learn a representation so that **examples from the same class have similar representations**\n",
    "\n",
    "- generatvie model : P(x), P(x,y) / unsupervised or generatvie criterion : -logP(x) or -logP(x,y)\n",
    "- discriminative model : P(y|x) / supervised criterion : -log P(y|x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7. Multi-Task Learining\n",
    "1. Task-speciﬁc parameters (which only beneﬁt from the examples of their task to achieve good generalization)\n",
    "2. Generic parameters, shared across all the tasks (which beneﬁt from the pooled data of all the tasks)\n",
    "\n",
    " the underlying prior belief is the following: among the factors that explain the variations observed in the data associated with the diﬀerent tasks, some are shared across two or more tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8. Early Stopping\n",
    "\n",
    "Every time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-speciﬁed number of iterations\n",
    "\n",
    "- One way to think of early stopping is as a **very eﬃcient hyperparameter selection algorithm. **\n",
    "\n",
    "- it is easy to use early stopping **without damaging the learning dynamics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How early stopping acts as a regularizer:**\n",
    "\n",
    "-  (1995) early stopping has the eﬀect of restricting the optimization procedure to a relatively small volume of parameter space in the neighborhood of the initial parameter value \n",
    "\n",
    "\n",
    "- $\\tau$ optimization steps (corresponding to $\\tau$ training iterations) \n",
    "- learning rate $\\epsilon$\n",
    "- **We can view the product $\\epsilon\\tau$ as a measure of eﬀective capacity**\n",
    "\n",
    "\n",
    "- ** early stopping is equivalent to L2 regularization. **\n",
    "- $ (I-\\epsilon \\Lambda)^\\tau = (\\Lambda + \\alpha I)^{-1}\\alpha$\n",
    "- $\\alpha \\approx \\cfrac{1}{\\tau\\epsilon}$\n",
    "-  **the inverse of $\\tau\\epsilon$ plays the role of the weight decay coeﬃcient. **\n",
    "\n",
    "\n",
    "-  early stopping automatically determines the correct amount of regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9. Parameter Tying and Parameter Sharing\n",
    "\n",
    "- a parameter norm penalty is one way to regularize parameters to be close to one anothe\n",
    "\n",
    "- <center>$\\Omega(w^{(A)}, w^{(B)}) = ||w^{(A)} -  w^{(B)}||^2_2$\n",
    "\n",
    "- the more popular way is to use constraints: **to force sets of parameters to be equal. ==> parameter sharing **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.10. Sparse Representation\n",
    "\n",
    "## 7.11. Bagging and Other Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12. Dropout\n",
    "\n",
    "- To a ﬁrst approximation, dropout can be thought of as ** a method of making bagging practical for ensembles** of very many large neural networks\n",
    "- we can eﬀectively remove a unit from a network by ** multiplying its output value by zero**\n",
    "-  Speciﬁcally, to train with dropout, we use a minibatch-based learning algorithm that makes small steps, such as stochastic gradient descent\n",
    "- Each time we load an example into a minibatch, we randomly sample a ** diﬀerent binary mask to apply to all of the input and hidden units in the network**\n",
    "- In the case of bagging, the models are all independent. \n",
    "- In the case of **dropout, the models share parameters.** This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memor\n",
    "- In the case of bagging,each model i produces a probability distribution $ p^{(i)} (y | x)$. The prediction of the ensemble is given by the arithmetic mean of all of these distributions. $ \\cfrac{1}{k}\\sum_{i=1}^k p^{(i)}(y|x) $\n",
    "- In the case of dropout, each sub-model deﬁned by mask vector µ deﬁnes a probability distribution $ p(y | x,µ). $ The arithmetic mean over all masks is given by $ \\sum_\\mu p(\\mu)p(y|x,\\mu) $ where p(µ) is the probability distribution that was used to sample µ at training time. \n",
    "- To make predictions we must re-normalizae the ensemble:\n",
    "$ P_{ensamble}(y|x) = \\cfrac{\\tilde{P}_{ensemble}(y|x)}{\\sum_{y'}\\tilde{P}_{ensemble}(y'|x)} $\n",
    "- ** A key insight involved in dropout is that we can approximate ensemble by evaluating p(y | x) in one model **\n",
    "- One advantage of dropout is that it is very computationally cheap\n",
    "- Another signiﬁcant advantage of dropout is that it does not signiﬁcantly limit the type of model or training procedure that can be used.\n",
    "- It is important to understand that a large portion of the power of dropout arises from the fact that **the masking noise is applied to the hidden units**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.13. Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.14. Tangent Distance, Tangent Prop, and Manifold Tangent Classiﬁer\n",
    "\n",
    "- One of the early attempts to take advantage of the manifold hypothesis is the tangent distance algorithm \n",
    "-  It is a non-parametric nearest-neighbor algorithm in which the metric used is **not the generic Euclidean distance** but one that is derived from knowledge of the **manifolds near which probability concentrates.**\n",
    "\n",
    "\n",
    "manifold tangent classiﬁer\n",
    "\n",
    "- (1) use an autoencoder to learn the manifold structure by unsupervised learning, and\n",
    "- (2) use these tangents to regularize a neural net classiﬁer as in tangent pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
